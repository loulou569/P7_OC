{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P7_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONujy+VCIg7OfHZvrng6jS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loulou569/P7_OC/blob/main/P7_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG4DCSH4eIay"
      },
      "source": [
        "connection au drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RinlG_nUd4W0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e1f4f3-63c5-48b8-f967-8b2012e3c5ef"
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmJs_CZ8J-In"
      },
      "source": [
        "# Github export commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_rTS19Buf0A"
      },
      "source": [
        "déplacement dans le dossier Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2giuAOZOtqJ6",
        "outputId": "c5d419a5-2022-485f-a9fa-d08bc7b98438"
      },
      "source": [
        "# %cd /content/gdrive/My Drive/Github"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Github\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKlZo4M-ySJN"
      },
      "source": [
        "configuration de git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67a7ZmviyZgu"
      },
      "source": [
        "# !git config --global user.name \"loulou569\"\n",
        "# !git config --global user.email \"thomlg2@hotmail.com\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux9iOmasw9X3"
      },
      "source": [
        "initilisation du dépot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfs498akwhsE",
        "outputId": "1ec7ecfe-da85-482f-e043-75bbff950b7e"
      },
      "source": [
        "# !git init P7_OC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/gdrive/My Drive/Github/P7_OC/.git/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Z83YEHxTPx"
      },
      "source": [
        "déplacement dans le dossier du projet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmI1z-9Zwhpk",
        "outputId": "9c7a35c7-53ab-4907-c0dd-94d322fcf1b0"
      },
      "source": [
        "# %cd P7_OC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Github/P7_OC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v02hMZZzxYYB"
      },
      "source": [
        "pour voir le statut des fichiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_8jCqX-whnJ",
        "outputId": "0459de43-4109-4d9f-bbc0-e9d1c0dec652"
      },
      "source": [
        "# !git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "\n",
            "No commits yet\n",
            "\n",
            "nothing to commit (create/copy files and use \"git add\" to track)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWFNjcY8xgKV"
      },
      "source": [
        "pour rajouter les changements dans le 'stage'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaHbW949xbpH"
      },
      "source": [
        "# !git add ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_mweXE3yBoo"
      },
      "source": [
        "envoie les commit vers le repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSNFjvtHyB6O",
        "outputId": "3abe7380-bcde-4a67-8754-2367bfafb3e4"
      },
      "source": [
        "# !git commit -m 'Première version'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "\n",
            "Initial commit\n",
            "\n",
            "nothing to commit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihn_9uJh0vhB"
      },
      "source": [
        "envoie vers Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It191FAv2djF"
      },
      "source": [
        "# !git remote remove origin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W4LQTJXw6pM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "956f0d6f-1d72-450a-d752-accb670bcd0e"
      },
      "source": [
        "# !git remote add origin https://github.com/loulou569/P7_OC.git\n",
        "# !git remote add origin https://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@github.com/loulou569/P7_OC.git\n",
        "# !git remote -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: remote origin already exists.\n",
            "origin\thttps://github.com/loulou569/P7_OC.git (fetch)\n",
            "origin\thttps://github.com/loulou569/P7_OC.git (push)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kb2R6hI5o2g",
        "outputId": "14e290af-2a16-4986-abe9-68fdad9bcc83"
      },
      "source": [
        "# !git branch -M main"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error: refname refs/heads/master not found\n",
            "fatal: Branch rename failed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSjQJk9K0871",
        "outputId": "93997c0a-a30f-44d1-abae-570e9c23b782"
      },
      "source": [
        "# !git branch -M main\n",
        "# !git push -u origin main"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error: refname refs/heads/master not found\n",
            "fatal: Branch rename failed\n",
            "error: src refspec master does not match any.\n",
            "error: failed to push some refs to 'https://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@github.com/loulou569/P7_OC.git'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqSs0BuWKCbu"
      },
      "source": [
        "# notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZK-NmlYK5PU"
      },
      "source": [
        "import de packages/fonctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocl3I_ZDK5n9"
      },
      "source": [
        "# mettre le path où se trouve le fichier avec les fonctions\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpJfA9KeDMFw"
      },
      "source": [
        "import des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ565M_bDNnT"
      },
      "source": [
        "dict_data = {}\n",
        "\n",
        "dict_data[\"application_test\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/application_test.csv')\n",
        "\n",
        "dict_data[\"application_train\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/application_train.csv')\n",
        "\n",
        "dict_data[\"bureau\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/bureau.csv')\n",
        "\n",
        "dict_data[\"bureau_balance\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/bureau_balance.csv')\n",
        "\n",
        "dict_data[\"credit_card_balance\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/credit_card_balance.csv')\n",
        "\n",
        "dict_data[\"homecredit_columns_description\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/HomeCredit_columns_description.csv', \n",
        "     encoding='cp437')\n",
        "\n",
        "dict_data[\"installments_payments\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/installments_payments.csv')\n",
        "\n",
        "dict_data[\"pos_cash_balance\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/POS_CASH_balance.csv')\n",
        "\n",
        "dict_data[\"previous_application\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/previous_application.csv')\n",
        "\n",
        "dict_data[\"sample_submission\"] =  pd.read_csv(\n",
        "    '/content/gdrive/My Drive/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im-4sxKhrzzH"
      },
      "source": [
        "affichage du nombre de lignes et colonnes de chaque dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh_utSnHKO_G",
        "outputId": "16a09469-1391-4a42-d4c9-50bff31fb527"
      },
      "source": [
        "for dataset in dict_data.keys():\n",
        "    n_line = dict_data[dataset].shape[0]\n",
        "    n_col = dict_data[dataset].shape[1]\n",
        "    print(\"Le dataset {} a {} lignes et {} colonnes\".\n",
        "          format(dataset, n_line, n_col))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Le dataset application_test a 48744 lignes et 121 colonnes\n",
            "Le dataset application_train a 307511 lignes et 122 colonnes\n",
            "Le dataset bureau a 1716428 lignes et 17 colonnes\n",
            "Le dataset bureau_balance a 27299925 lignes et 3 colonnes\n",
            "Le dataset credit_card_balance a 3840312 lignes et 23 colonnes\n",
            "Le dataset homecredit_columns_description a 219 lignes et 5 colonnes\n",
            "Le dataset installments_payments a 13605401 lignes et 8 colonnes\n",
            "Le dataset pos_cash_balance a 10001358 lignes et 8 colonnes\n",
            "Le dataset previous_application a 1670214 lignes et 37 colonnes\n",
            "Le dataset sample_submission a 48744 lignes et 2 colonnes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYs2zBIWsL2s"
      },
      "source": [
        "détection de la colonne non commune entre les datasets \"appilcation_test\" et \"application_train\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAVoCqQ_IcHF",
        "outputId": "dac27207-1236-43c9-e1d6-6abd8d719521"
      },
      "source": [
        "col_test = np.array(dict_data['application_test'].columns)\n",
        "col_train = np.array(dict_data['application_train'].columns)\n",
        "\n",
        "\n",
        "diff = np.setdiff1d(col_train, col_test)\n",
        "if len(diff) > 0:\n",
        "    print(\"La/les colonne(s) {} du dataset application_train n'existe(nt) pas \\\n",
        "dans le dataset application_test\".format(diff[0]))\n",
        "\n",
        "diff = np.setdiff1d(col_test, col_train)\n",
        "if len(diff) > 0:\n",
        "    print(\"La/les colonne(s) {} du dataset application_test n'existe(nt) pas \\\n",
        "dans le dataset application_train\".format(diff[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La/les colonne(s) TARGET du dataset application_train n'existe(nt) pas dans le dataset application_test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEnpZXPlqj6t"
      },
      "source": [
        "détection des colonnes non communes entre les datasets \"application_train\" et les autres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf52R5wrrylZ",
        "outputId": "e378b578-2339-466d-b848-b8a55dc40d82"
      },
      "source": [
        "list_sum = [\"application_test\", \n",
        "            \"application_train\", \n",
        "            \"homecredit_columns_description\"]\n",
        "\n",
        "list_columns = np.array([])\n",
        "\n",
        "for dataset in dict_data.keys():\n",
        "    if dataset not in list_sum:\n",
        "        list_columns = np.concatenate(\n",
        "            (list_columns, np.array(dict_data[dataset].columns))\n",
        "            )\n",
        "list_columns = np.unique(list_columns)       \n",
        "\n",
        "col_train = np.array(dict_data['application_train'].columns)\n",
        "\n",
        "diff = np.setdiff1d(col_train, list_columns)\n",
        "if len(diff) > 0:\n",
        "    print(\"{} colonne(s) du dataset application_train n'existe(nt) pas \\\n",
        "dans les autres dataset\\n\".format(diff.size))\n",
        "    \n",
        "diff = np.setdiff1d(list_columns, col_train)\n",
        "if len(diff) > 0:\n",
        "    print(\"{} colonne(s) des autres dataset n'existe(nt) pas \\\n",
        "dans le dataset application_train\".format(diff.size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113 colonne(s) du dataset application_train n'existe(nt) pas dans les autres dataset\n",
            "\n",
            "73 colonne(s) des autres dataset n'existe(nt) pas dans le dataset application_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svw4oJ5hrj6g"
      },
      "source": [
        "vérification de l'égalité entre les index de 'application_test' et 'sample_submission'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM6o3hXUugbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2e4526-aba1-4d56-9f5d-59687b15ea54"
      },
      "source": [
        "test = \\\n",
        "list(dict_data['application_test'].index) ==\\\n",
        "list(dict_data['sample_submission'].index)\n",
        "\n",
        "if test: \n",
        "    print('Les index des dataset \"application_test\" et \"sample_submission\" \\\n",
        "sont identiques')\n",
        "else: \n",
        "    print('Les index des dataset \"application_test\" et \"sample_submission\" \\\n",
        "ne sont pas identiques')\n",
        "    \n",
        "\n",
        "test = \\\n",
        "list(dict_data['application_test']['SK_ID_CURR']) ==\\\n",
        "list(dict_data['sample_submission']['SK_ID_CURR'])\n",
        "print(test)\n",
        "\n",
        "if test: \n",
        "    print('Les SK_ID_CURR des dataset \"application_test\" et \"sample_submission\" \\\n",
        "sont identiques')\n",
        "else: \n",
        "    print('Les SK_ID_CURR des dataset \"application_test\" et \"sample_submission\" \\\n",
        "ne sont pas identiques')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Les index des dataset \"application_test\" et \"sample_submission\" sont identiques\n",
            "True\n",
            "Les SK_ID_CURR des dataset \"application_test\" et \"sample_submission\" sont identiques\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0HziBdFvrYx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkv01IdblTxK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gaMfjlAlTsX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNFFFq2SlTiZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRchjPBTV7pd"
      },
      "source": [
        "# extrait du notebook \"HomeCredit Default Risk Step by Step: 1st Notebook\" de Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N82M_9sRWgjO"
      },
      "source": [
        "functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXpXbVxlWZVM"
      },
      "source": [
        "# Reduce Memory Usage\n",
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            \n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# One-hot encoding for categorical columns with get_dummies\n",
        "def one_hot_encoder(df, nan_as_category=True):\n",
        "    original_columns = list(df.columns)\n",
        "    categorical_columns = df.select_dtypes([\"category\", \"object\"]).columns.tolist()\n",
        "    # categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\n",
        "    return df, new_columns\n",
        "\n",
        "# Grab Column Names\n",
        "def grab_col_names(dataframe, cat_th=10, car_th=20, show_date=False):\n",
        "    date_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"datetime64[ns]\"]\n",
        "\n",
        "    #cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
        "    cat_cols = dataframe.select_dtypes([\"object\", \"category\"]).columns.tolist()\n",
        "    \n",
        "    \n",
        "    \n",
        "    num_but_cat = [col for col in dataframe.select_dtypes([\"float\", \"integer\"]).columns if dataframe[col].nunique() < cat_th]\n",
        "\n",
        "    cat_but_car = [col for col in dataframe.select_dtypes([\"object\", \"category\"]).columns if dataframe[col].nunique() > car_th]\n",
        "\n",
        "    cat_cols = cat_cols + num_but_cat\n",
        "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
        "\n",
        "    num_cols = dataframe.select_dtypes([\"float\", \"integer\"]).columns\n",
        "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
        "\n",
        "    print(f\"Observations: {dataframe.shape[0]}\")\n",
        "    print(f\"Variables: {dataframe.shape[1]}\")\n",
        "    print(f'date_cols: {len(date_cols)}')\n",
        "    print(f'cat_cols: {len(cat_cols)}')\n",
        "    print(f'num_cols: {len(num_cols)}')\n",
        "    print(f'cat_but_car: {len(cat_but_car)}')\n",
        "    print(f'num_but_cat: {len(num_but_cat)}')\n",
        "\n",
        "    # cat_cols + num_cols + cat_but_car = değişken sayısı.\n",
        "    # num_but_cat cat_cols'un içerisinde zaten.\n",
        "    # dolayısıyla tüm şu 3 liste ile tüm değişkenler seçilmiş olacaktır: cat_cols + num_cols + cat_but_car\n",
        "    # num_but_cat sadece raporlama için verilmiştir.\n",
        "\n",
        "    if show_date == True:\n",
        "        return date_cols, cat_cols, cat_but_car, num_cols, num_but_cat\n",
        "    else:\n",
        "        return cat_cols, cat_but_car, num_cols, num_but_cat\n",
        "\n",
        "# Categorical Variables & Target\n",
        "def cat_analyzer(dataframe, variable, target = None):\n",
        "    print(variable)\n",
        "    if target == None:\n",
        "        print(pd.DataFrame({\n",
        "            \"COUNT\": dataframe[variable].value_counts(),\n",
        "            \"RATIO\": dataframe[variable].value_counts() / len(dataframe)}), end=\"\\n\\n\\n\")\n",
        "    else:\n",
        "        temp = dataframe[dataframe[target].isnull() == False]\n",
        "        print(pd.DataFrame({\n",
        "            \"COUNT\":dataframe[variable].value_counts(),\n",
        "            \"RATIO\":dataframe[variable].value_counts() / len(dataframe),\n",
        "            \"TARGET_COUNT\":dataframe.groupby(variable)[target].count(),\n",
        "            \"TARGET_MEAN\":temp.groupby(variable)[target].mean(),\n",
        "            \"TARGET_MEDIAN\":temp.groupby(variable)[target].median(),\n",
        "            \"TARGET_STD\":temp.groupby(variable)[target].std()}), end=\"\\n\\n\\n\")\n",
        "\n",
        "# Numerical Variables\n",
        "def corr_plot(data, remove=[\"Id\"], corr_coef = \"pearson\", figsize=(20, 20)):\n",
        "    if len(remove) > 0:\n",
        "        num_cols2 = [x for x in data.columns if (x not in remove)]\n",
        "\n",
        "    sns.set(font_scale=1.1)\n",
        "    c = data[num_cols2].corr(method = corr_coef)\n",
        "    mask = np.triu(c.corr(method = corr_coef))\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(c,\n",
        "                annot=True,\n",
        "                fmt='.1f',\n",
        "                cmap='coolwarm',\n",
        "                square=True,\n",
        "                mask=mask,\n",
        "                linewidths=1,\n",
        "                cbar=False)\n",
        "    plt.show()\n",
        "\n",
        "# Plot numerical variables\n",
        "def num_plot(data, num_cols, remove=[\"Id\"], hist_bins=10, figsize=(20, 4)):\n",
        "\n",
        "    if len(remove) > 0:\n",
        "        num_cols2 = [x for x in num_cols if (x not in remove)]\n",
        "\n",
        "    for i in num_cols2:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "        data.hist(str(i), bins=hist_bins, ax=axes[0])\n",
        "        data.boxplot(str(i), ax=axes[1], vert=False);\n",
        "        try:\n",
        "            sns.kdeplot(np.array(data[str(i)]))\n",
        "        except:\n",
        "            ValueError\n",
        "\n",
        "        axes[1].set_yticklabels([])\n",
        "        axes[1].set_yticks([])\n",
        "        axes[0].set_title(i + \" | Histogram\")\n",
        "        axes[1].set_title(i + \" | Boxplot\")\n",
        "        axes[2].set_title(i + \" | Density\")\n",
        "        plt.show()\n",
        "\n",
        "# Get high correlated variables\n",
        "def high_correlation(data, remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.7):\n",
        "    if len(remove) > 0:\n",
        "        cols = [x for x in data.columns if (x not in remove)]\n",
        "        c = data[cols].corr(method=corr_coef)\n",
        "    else:\n",
        "        c = data.corr(method=corr_coef)\n",
        "\n",
        "    for i in c.columns:\n",
        "        cr = c.loc[i].loc[(c.loc[i] >= corr_value) | (c.loc[i] <= -corr_value)].drop(i)\n",
        "        if len(cr) > 0:\n",
        "            print(i)\n",
        "            print(\"-------------------------------\")\n",
        "            print(cr.sort_values(ascending=False))\n",
        "            print(\"\\n\")\n",
        "\n",
        "# Missing Value\n",
        "def missing_values(data, plot=False):\n",
        "    mst = pd.DataFrame(\n",
        "        {\"Num_Missing\": data.isnull().sum(), \"Missing_Ratio\": data.isnull().sum() / data.shape[0]}).sort_values(\n",
        "        \"Num_Missing\", ascending=False)\n",
        "    mst[\"DataTypes\"] = data[mst.index].dtypes.values\n",
        "    mst = mst[mst.Num_Missing > 0].reset_index().rename({\"index\": \"Feature\"}, axis=1)\n",
        "\n",
        "    print(\"Number of Variables include Missing Values:\", mst.shape[0], \"\\n\")\n",
        "\n",
        "    if mst[mst.Missing_Ratio >= 1.0].shape[0] > 0:\n",
        "        print(\"Full Missing Variables:\", mst[mst.Missing_Ratio >= 1.0].Feature.tolist())\n",
        "        data.drop(mst[mst.Missing_Ratio >= 1.0].Feature.tolist(), axis=1, inplace=True)\n",
        "\n",
        "        print(\"Full missing variables are deleted!\", \"\\n\")\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(25, 8))\n",
        "        p = sns.barplot(mst.Feature, mst.Missing_Ratio)\n",
        "        for rotate in p.get_xticklabels():\n",
        "            rotate.set_rotation(90)\n",
        "        plt.show()\n",
        "\n",
        "    print(mst, \"\\n\")\n",
        "    \n",
        "    \n",
        "# Quantile functions for aggregations\n",
        "def quantile_funcs(percentiles = [0.75, 0.9, 0.99]):\n",
        "    return [(p, lambda x: x.quantile(p)) for p in percentiles]\n",
        "\n",
        "# Rare Encoder\n",
        "def rare_encoder(data, col, rare_perc):\n",
        "    temp = data[col].value_counts() / len(data) < rare_perc\n",
        "    data[col] = np.where(~data[col].isin(temp[temp < rare_perc].index), \"Rare\", data[col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjYi4uiRugiy"
      },
      "source": [
        "feature engineering bureau et bureaubalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eawJlZU9unYF"
      },
      "source": [
        "bureau_balance = pd.read_csv(\"../input/home-credit-default-risk/bureau_balance.csv\")\n",
        "bureau_balance = reduce_mem_usage(bureau_balance)\n",
        "\n",
        "print(bureau_balance.shape, \"\\n\")\n",
        "\n",
        "bureau_balance.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Oru2bwvkDj"
      },
      "source": [
        "# One-Hot Encoder\n",
        "bb, bb_cat = one_hot_encoder(bureau_balance, nan_as_category=False)\n",
        "\n",
        "# Bureau balance: Perform aggregations and merge with bureau.csv\n",
        "bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
        "\n",
        "for col in bb_cat:\n",
        "    bb_aggregations[col] = ['mean']\n",
        "\n",
        "bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
        "bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
        "\n",
        "# Status Sum\n",
        "bb_agg[\"STATUS_C0_MEAN_SUM\"] = bb_agg[[\"STATUS_C_MEAN\", \"STATUS_0_MEAN\"]].sum(axis = 1)\n",
        "bb_agg[\"STATUS_12_MEAN_SUM\"] = bb_agg[[\"STATUS_1_MEAN\", \"STATUS_2_MEAN\"]].sum(axis = 1)\n",
        "bb_agg[\"STATUS_345_MEAN_SUM\"] = bb_agg[[\"STATUS_3_MEAN\", \"STATUS_4_MEAN\", \"STATUS_5_MEAN\"]].sum(axis = 1)\n",
        "bb_agg[\"STATUS_12345_MEAN_SUM\"] = bb_agg[[\"STATUS_1_MEAN\", \"STATUS_2_MEAN\", \"STATUS_3_MEAN\", \"STATUS_4_MEAN\", \"STATUS_5_MEAN\"]].sum(axis = 1)\n",
        "\n",
        "# Find the first month when the credit is closed!\n",
        "closed = bureau_balance[bureau_balance.STATUS == \"C\"]\n",
        "closed = closed.groupby(\"SK_ID_BUREAU\").MONTHS_BALANCE.min().reset_index().rename({\"MONTHS_BALANCE\":\"MONTHS_BALANCE_FIRST_C\"}, axis = 1)\n",
        "closed[\"MONTHS_BALANCE_FIRST_C\"] = np.abs(closed[\"MONTHS_BALANCE_FIRST_C\"])\n",
        "bb_agg = pd.merge(bb_agg, closed, how = \"left\", on = \"SK_ID_BUREAU\")\n",
        "bb_agg[\"MONTHS_BALANCE_CLOSED_DIF\"] = np.abs(bb_agg.MONTHS_BALANCE_MIN) - bb_agg.MONTHS_BALANCE_FIRST_C\n",
        "\n",
        "del closed, bb_aggregations, bureau_balance, bb_cat\n",
        "\n",
        "print(\"BURAU BALANCE SHAPE:\", bb_agg.shape, \"\\n\")\n",
        "\n",
        "bb_agg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTDJeYs8vlEL"
      },
      "source": [
        "bureau = pd.read_csv(\"../input/home-credit-default-risk/bureau.csv\")\n",
        "bureau = reduce_mem_usage(bureau)\n",
        "\n",
        "print(bureau.shape, \"\\n\")\n",
        "\n",
        "bureau.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTVFbBqMvniV"
      },
      "source": [
        "# LEFT JOIN WITH BUREAU\n",
        "bureau = pd.merge(bureau, bb_agg, how='left', on='SK_ID_BUREAU')\n",
        "del bb_agg\n",
        "\n",
        "print(bureau.shape, \"\\n\")\n",
        "\n",
        "bureau.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC5pGW7tv-C5"
      },
      "source": [
        "# Columns\n",
        "cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(bureau, car_th=10)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Categorical Features\n",
        "print(cat_cols, cat_but_car)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHBIgtU3v__z"
      },
      "source": [
        "# Cat Analyzer\n",
        "for i in cat_cols + cat_but_car:\n",
        "    cat_analyzer(bureau, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0bj0XCowKQW"
      },
      "source": [
        "# FEATURE ENGINEERING FOR BUREAU\n",
        "\n",
        "# Categorical Variables\n",
        "# -----------------------------------------------------------\n",
        "# Useless\n",
        "bureau.drop(\"CREDIT_CURRENCY\", axis = 1, inplace = True)\n",
        "\n",
        "# Rare Categories\n",
        "bureau[\"CREDIT_ACTIVE\"] = np.where(bureau.CREDIT_ACTIVE.isin([\"Sold\", \"Bad debt\"]), \"Sold_BadDebt\", bureau.CREDIT_ACTIVE)\n",
        "\n",
        "bureau[\"CREDIT_TYPE\"] = np.where(\n",
        "    ~bureau.CREDIT_TYPE.isin(\n",
        "        [\"Consumer credit\", \"Credit card\", \"Car loan\", \"Mortgage\", \"Microloan\"]\n",
        "    ), \"Other\", bureau[\"CREDIT_TYPE\"])\n",
        "\n",
        "# One-Hot Encoder\n",
        "bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category=False)\n",
        "\n",
        "\n",
        "# Numerical Variables\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Bureau and bureau_balance numeric features\n",
        "cal = ['min', 'max', 'mean', 'sum', 'median','std']\n",
        "cols1 = [\n",
        "    'DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_CREDIT_UPDATE','CREDIT_DAY_OVERDUE',\n",
        "    'AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_OVERDUE',\n",
        "    'AMT_CREDIT_SUM_LIMIT', 'AMT_ANNUITY', 'CNT_CREDIT_PROLONG', 'MONTHS_BALANCE_MIN',\n",
        "    'MONTHS_BALANCE_MAX', 'MONTHS_BALANCE_SIZE', 'MONTHS_BALANCE_FIRST_C', 'MONTHS_BALANCE_CLOSED_DIF'\n",
        "]\n",
        "\n",
        "num_aggregations = {}\n",
        "\n",
        "\n",
        "for i in cols1:\n",
        "    num_aggregations[i] = cal\n",
        "    \n",
        "    \n",
        "# Bureau and bureau_balance categorical features\n",
        "cat_aggregations = {}\n",
        "\n",
        "for i in bureau_cat:\n",
        "    cat_aggregations[i] = ['mean']\n",
        "\n",
        "cols2 = ['STATUS_0_MEAN', 'STATUS_1_MEAN', 'STATUS_2_MEAN', 'STATUS_3_MEAN', 'STATUS_4_MEAN',\n",
        "        'STATUS_5_MEAN', 'STATUS_C_MEAN', 'STATUS_X_MEAN', 'STATUS_C0_MEAN_SUM',\n",
        "        'STATUS_12_MEAN_SUM', 'STATUS_345_MEAN_SUM', 'STATUS_12345_MEAN_SUM']\n",
        "for i in cols2:\n",
        "    cat_aggregations[i] = ['mean', 'median', 'sum', 'max', 'std']\n",
        "\n",
        "del i, cols1, cols2, bureau_cat, cal\n",
        "    \n",
        "# Create aggregated data\n",
        "bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
        "bureau_agg.columns = pd.Index(['BUREAU_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
        "\n",
        "\n",
        "# New features\n",
        "bureau.groupby(\"SK_ID_CURR\").SK_ID_BUREAU.count().value_counts()\n",
        "bcount = bureau.groupby(\"SK_ID_CURR\").SK_ID_BUREAU.count().reset_index().rename({\"SK_ID_BUREAU\":\"BUREAU_COUNT\"}, axis = 1)\n",
        "bcount[\"BUREAU_COUNT_CAT\"] = np.where(bcount.BUREAU_COUNT < 4, 0, 1)\n",
        "bcount[\"BUREAU_COUNT_CAT\"] = np.where((bcount.BUREAU_COUNT >= 8) & (bcount.BUREAU_COUNT < 13), 2, bcount[\"BUREAU_COUNT_CAT\"])\n",
        "bcount[\"BUREAU_COUNT_CAT\"] = np.where((bcount.BUREAU_COUNT >= 13) & (bcount.BUREAU_COUNT < 20), 3, bcount[\"BUREAU_COUNT_CAT\"])\n",
        "bcount[\"BUREAU_COUNT_CAT\"] = np.where((bcount.BUREAU_COUNT >= 20), 4, bcount[\"BUREAU_COUNT_CAT\"])\n",
        "bureau_agg = pd.merge(bureau_agg, bcount, how = \"left\", on = \"SK_ID_CURR\")\n",
        "del bcount\n",
        "\n",
        "\n",
        "# Bureau: Active credits - using only numerical aggregations\n",
        "active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
        "active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
        "bureau_agg = pd.merge(bureau_agg, active_agg, how='left', on='SK_ID_CURR')\n",
        "del active, active_agg\n",
        "\n",
        "\n",
        "# Bureau: Closed credits - using only numerical aggregations\n",
        "closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
        "closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
        "bureau_agg = pd.merge(bureau_agg, closed_agg, how='left', on='SK_ID_CURR')\n",
        "del closed, closed_agg\n",
        "\n",
        "# Bureau: Sold and Bad Debt credits - using only numerical aggregations\n",
        "sold_baddebt = bureau[bureau['CREDIT_ACTIVE_Sold_BadDebt'] == 1]\n",
        "sold_baddebt_agg = sold_baddebt.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "sold_baddebt_agg.columns = pd.Index(['SOLD_BADDEBT' + e[0] + \"_\" + e[1].upper() for e in sold_baddebt_agg.columns.tolist()])\n",
        "bureau_agg = pd.merge(bureau_agg, sold_baddebt_agg, how='left', on='SK_ID_CURR')\n",
        "del sold_baddebt, sold_baddebt_agg, bureau\n",
        "\n",
        "del num_aggregations, cat_aggregations\n",
        "\n",
        "\n",
        "# WRITE FEATHER\n",
        "bureau_agg.to_feather(\"bureau_bureaubalance_agg_feather\")\n",
        "#pd.read_feather(\"./bureau_bureaubalance_agg_feather\")\n",
        "\n",
        "print(\"BUREAU & BURAU BALANCE SHAPE:\", bureau_agg.shape, \"\\n\")\n",
        "\n",
        "bureau_agg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z5NFxyGsq9w"
      },
      "source": [
        "feature engineering poscashblance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz25NRRbswhz"
      },
      "source": [
        "pos = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\n",
        "pos = reduce_mem_usage(pos)\n",
        "\n",
        "print(pos.shape, \"\\n\")\n",
        "\n",
        "# Columns\n",
        "cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(pos, car_th=10)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "pos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z9J_CpstXuG"
      },
      "source": [
        "# Rare\n",
        "pos[\"NAME_CONTRACT_STATUS\"] = np.where(~(pos[\"NAME_CONTRACT_STATUS\"].isin([\n",
        "   \"Active\", \"Completed\"\n",
        "])), \"Rare\", pos[\"NAME_CONTRACT_STATUS\"])\n",
        "\n",
        "# One-Hot Encoder\n",
        "pos, cat_cols = one_hot_encoder(pos, nan_as_category=False)\n",
        "\n",
        "aggregations = {\n",
        "    # Numerical\n",
        "    'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
        "    'CNT_INSTALMENT': ['max', 'mean', 'std', 'min', 'median'],\n",
        "    'CNT_INSTALMENT_FUTURE': ['max', 'mean', 'sum', 'min', 'median', 'std'],\n",
        "    'SK_DPD': ['max', 'mean'],\n",
        "    'SK_DPD_DEF': ['max', 'mean']\n",
        "}\n",
        "# Categorical\n",
        "for cat in cat_cols:\n",
        "    aggregations[cat] = ['mean']\n",
        "\n",
        "# Aggregation\n",
        "pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
        "pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
        "# Count pos cash accounts\n",
        "pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
        "pos_agg.reset_index(inplace = True)\n",
        "del pos\n",
        "\n",
        "print(\"POS CASH BALANCE SHAPE:\", pos_agg.shape, \"\\n\")\n",
        "\n",
        "pos_agg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1UkFP6Iw4GB"
      },
      "source": [
        "feature engineering credit cardbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMXM_eYZw8Ma"
      },
      "source": [
        "cc = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\n",
        "cc = reduce_mem_usage(cc)\n",
        "\n",
        "print(cc.shape, \"\\n\")\n",
        "\n",
        "# Columns\n",
        "cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(cc, car_th=10)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "cc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7dRf9x5xJIW"
      },
      "source": [
        "# Rare\n",
        "cc[\"NAME_CONTRACT_STATUS\"] = np.where(~(cc[\"NAME_CONTRACT_STATUS\"].isin([\n",
        "   \"Active\", \"Completed\"\n",
        "])), \"Rare\", cc[\"NAME_CONTRACT_STATUS\"])\n",
        "\n",
        "# One Hot Encoder\n",
        "cc, cat_cols = one_hot_encoder(cc, nan_as_category=False)\n",
        "\n",
        "# General aggregations\n",
        "cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
        "cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'std'])\n",
        "cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
        "# Count credit card lines\n",
        "cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
        "cc_agg.reset_index(inplace = True)\n",
        "del cc\n",
        "\n",
        "print(\"CREDIT CARD BALANCE SHAPE:\", cc_agg.shape, \"\\n\")\n",
        "\n",
        "cc_agg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVK1ElSHz7iC"
      },
      "source": [
        "feature engineering installments_payments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWQ4AKT-z76I"
      },
      "source": [
        "ins = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv')\n",
        "ins = reduce_mem_usage(ins)\n",
        "\n",
        "print(ins.shape, \"\\n\")\n",
        "\n",
        "# Columns\n",
        "cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(ins, car_th=10)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "ins.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P52EC3Lg0Yq7"
      },
      "source": [
        "# Percentage and difference paid in each installment (amount paid and installment value)\n",
        "ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
        "ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
        "# Days past due and days before due (no negative values)\n",
        "ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
        "ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
        "ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
        "ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
        "# Features: Perform aggregations\n",
        "aggregations = {\n",
        "    'NUM_INSTALMENT_VERSION': ['nunique'],\n",
        "    'NUM_INSTALMENT_NUMBER': ['max', 'mean', 'sum', 'median', 'std'],\n",
        "    'DAYS_INSTALMENT': ['max', 'mean', 'sum', 'median', 'std'],\n",
        "    'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'median', 'std'],\n",
        "    'AMT_INSTALMENT': ['max', 'mean', 'sum', 'median', 'std'],\n",
        "    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum', 'median', 'std'],\n",
        "    'DPD': ['max', 'mean', 'sum', 'median', 'std'],\n",
        "    'DBD': ['max', 'mean', 'sum', 'median', 'std'],\n",
        "    'PAYMENT_PERC': ['max', 'mean', 'sum', 'std', 'median'],\n",
        "    'PAYMENT_DIFF': ['max', 'mean', 'sum', 'std', 'median']\n",
        "}\n",
        "\n",
        "ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
        "ins_agg.columns = pd.Index(['INS_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
        "# Count installments accounts\n",
        "ins_agg['INS_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
        "\n",
        "ins_agg.reset_index(inplace = True)\n",
        "del ins\n",
        "\n",
        "\n",
        "\n",
        "print(\"INSTALLMENTS PAYMENTS SHAPE:\", ins_agg.shape, \"\\n\")\n",
        "\n",
        "ins_agg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxQ-Le640rPc"
      },
      "source": [
        "feature engineering previous_applications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Au5E5a0uQx"
      },
      "source": [
        "prev = pd.read_csv('../input/home-credit-default-risk/previous_application.csv')\n",
        "prev = reduce_mem_usage(prev)\n",
        "\n",
        "print(prev.shape, \"\\n\")\n",
        "\n",
        "# Columns\n",
        "cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(prev, car_th=10)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "prev.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyPh0wuj09PG"
      },
      "source": [
        "# Rare Encoder\n",
        "rare_cols = [\n",
        "    \"NAME_PAYMENT_TYPE\", \"CODE_REJECT_REASON\", \"CHANNEL_TYPE\", \"NAME_GOODS_CATEGORY\",\n",
        "    \"NAME_SELLER_INDUSTRY\", \"NAME_TYPE_SUITE\" \n",
        "]\n",
        "\n",
        "for i in rare_cols:\n",
        "    rare_encoder(prev, i, rare_perc = 0.01)\n",
        "\n",
        "prev[\"NAME_CASH_LOAN_PURPOSE\"] = np.where(~prev[\"NAME_CASH_LOAN_PURPOSE\"].isin([\"XAP\", \"XNA\"]), \"Other\", prev[\"NAME_CASH_LOAN_PURPOSE\"])\n",
        "\n",
        "rare_encoder(prev, \"NAME_PORTFOLIO\", rare_perc = 0.1) \n",
        "\n",
        "# Cash, Pos, Card\n",
        "prev[\"PRODUCT_COMBINATION_CATS\"] = np.where(prev[\"PRODUCT_COMBINATION\"].str.contains(\"Cash\"), \"CASH\", \"POS\")\n",
        "prev[\"PRODUCT_COMBINATION_CATS\"] = np.where(prev[\"PRODUCT_COMBINATION\"].str.contains(\"Card\"), \"CARD\", prev[\"PRODUCT_COMBINATION_CATS\"])\n",
        "# New categorical variables\n",
        "prev[\"PRODUCT_COMBINATION_POS_WITH\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"without\"))), \"WITHOUT\", \"OTHER\")\n",
        "prev[\"PRODUCT_COMBINATION_POS_WITH\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"with interest\"))), \"WITH\", prev[\"PRODUCT_COMBINATION_POS_WITH\"])\n",
        "prev[\"PRODUCT_COMBINATION_POS_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"household\"))), \"household\", \"OTHER\")\n",
        "prev[\"PRODUCT_COMBINATION_POS_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"industry\"))), \"industry\", prev[\"PRODUCT_COMBINATION_POS_TYPE\"])\n",
        "prev[\"PRODUCT_COMBINATION_POS_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"mobile\"))), \"mobile\", prev[\"PRODUCT_COMBINATION_POS_TYPE\"])\n",
        "prev[\"PRODUCT_COMBINATION_POS_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"other\"))), \"posother\", prev[\"PRODUCT_COMBINATION_POS_TYPE\"])\n",
        "prev[\"PRODUCT_COMBINATION_CASH_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"Cash\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"X-Sell\"))), \"xsell\", \"OTHER\")\n",
        "prev[\"PRODUCT_COMBINATION_CASH_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"Cash\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"Street\"))), \"street\", prev[\"PRODUCT_COMBINATION_CASH_TYPE\"])\n",
        "\n",
        "\n",
        "# Useless\n",
        "prev.drop([\"WEEKDAY_APPR_PROCESS_START\", \"FLAG_LAST_APPL_PER_CONTRACT\", \"NFLAG_LAST_APPL_IN_DAY\", \"NFLAG_LAST_APPL_IN_DAY\"], axis = 1, inplace = True)\n",
        "\n",
        "# One-Hot Encoder\n",
        "prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
        "\n",
        "\n",
        "# Days 365.243 values -> nan\n",
        "prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
        "prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
        "prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
        "prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
        "prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
        "\n",
        "# Add feature: value ask / value received percentage\n",
        "prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
        "\n",
        "\n",
        "# Previous applications numeric features\n",
        "num_aggregations = {\n",
        "    'AMT_ANNUITY': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'AMT_APPLICATION': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'AMT_CREDIT': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'APP_CREDIT_PERC': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'AMT_GOODS_PRICE': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'RATE_DOWN_PAYMENT': ['min', 'max', 'mean', \"std\"],\n",
        "    'RATE_INTEREST_PRIMARY': ['min', 'max', 'mean', \"std\"],\n",
        "    'RATE_INTEREST_PRIVILEGED': ['min', 'max', 'mean', \"std\"],\n",
        "    'DAYS_DECISION': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'CNT_PAYMENT': ['mean', 'sum', \"median\", \"std\"],\n",
        "    'SELLERPLACE_AREA': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'DAYS_FIRST_DRAWING': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'DAYS_FIRST_DUE': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'DAYS_LAST_DUE': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    'DAYS_TERMINATION': ['min', 'max', 'mean', \"median\", \"std\"],\n",
        "    # Categorical\n",
        "    \"NFLAG_INSURED_ON_APPROVAL\": [\"mean\"]\n",
        "}\n",
        "# Previous applications categorical features\n",
        "cat_aggregations = {}\n",
        "for cat in cat_cols:\n",
        "    cat_aggregations[cat] = ['mean']\n",
        "\n",
        "prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
        "prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
        "\n",
        "# Previous Applications: Approved Applications - only numerical features\n",
        "approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
        "approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
        "prev_agg = pd.merge(prev_agg,approved_agg, how='left', on='SK_ID_CURR')\n",
        "\n",
        "# Previous Applications: Refused Applications - only numerical features\n",
        "refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
        "refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
        "prev_agg = pd.merge(prev_agg, refused_agg, how='left', on='SK_ID_CURR')\n",
        "\n",
        "del refused, refused_agg, approved, approved_agg, prev\n",
        "prev_agg.reset_index(inplace = True)\n",
        "\n",
        "\n",
        "print(\"PREVIOUS APPLICATIONS SHAPE:\", prev_agg.shape, \"\\n\")\n",
        "\n",
        "prev_agg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsRgvXo3WaLO"
      },
      "source": [
        "feature engineering traintest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB64fb5bXVce"
      },
      "source": [
        "df = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\n",
        "test_df = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\n",
        "\n",
        "print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
        "\n",
        "df = df.append(test_df)\n",
        "df = reduce_mem_usage(df)\n",
        "\n",
        "# Columns\n",
        "cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df, car_th=10)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho6gRXa_WHs5"
      },
      "source": [
        "# ERRORS\n",
        "df = df[~(df.CODE_GENDER.str.contains(\"XNA\"))]  \n",
        "df = df[df.NAME_FAMILY_STATUS != \"Unknown\"]  \n",
        "\n",
        "# DROP\n",
        "cols = [\"NAME_HOUSING_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"FONDKAPREMONT_MODE\", \"WALLSMATERIAL_MODE\", \"HOUSETYPE_MODE\",\n",
        "        \"EMERGENCYSTATE_MODE\",\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\",\"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\"]\n",
        "df.drop(cols, axis = 1, inplace = True)\n",
        "\n",
        "# REGION\n",
        "cols = [\"REG_REGION_NOT_LIVE_REGION\",\"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \"REG_CITY_NOT_LIVE_CITY\",\n",
        " \"REG_CITY_NOT_WORK_CITY\",\"LIVE_CITY_NOT_WORK_CITY\"]\n",
        "df[\"REGION\"] = df[cols].sum(axis = 1)\n",
        "df.drop(cols, axis = 1, inplace = True)\n",
        "\n",
        "# Drop FLAG_DOCUMENT \n",
        "df.drop(df.columns[df.columns.str.contains(\"FLAG_DOCUMENT\")], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "# RARE ENCODER\n",
        "df[\"NAME_EDUCATION_TYPE\"] = np.where(df.NAME_EDUCATION_TYPE == \"Academic degree\", \"Higher education\", df.NAME_EDUCATION_TYPE)\n",
        "\n",
        "\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Business Entity\"), \"Business Entity\", df.ORGANIZATION_TYPE)\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Industry\"), \"Industry\", df.ORGANIZATION_TYPE)\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Trade\"), \"Trade\", df.ORGANIZATION_TYPE)\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Transport\"), \"Transport\", df.ORGANIZATION_TYPE)\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"School\", \"Kindergarten\", \"University\"]), \"Education\", df.ORGANIZATION_TYPE)\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Emergency\",\"Police\", \"Medicine\",\"Goverment\", \"Postal\", \"Military\", \"Security Ministries\", \"Legal Services\"]), \"Public\", df.ORGANIZATION_TYPE)\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Bank\", \"Insurance\"]), \"Finance\", df.ORGANIZATION_TYPE)\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Realtor\", \"Housing\"]), \"House\", df.ORGANIZATION_TYPE)\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Hotel\", \"Restaurant\"]), \"HotelRestaurant\", df.ORGANIZATION_TYPE)\n",
        "df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Cleaning\",\"Electricity\", \"Telecom\", \"Mobile\", \"Advertising\", \"Religion\", \"Culture\"]), \"Other\", df.ORGANIZATION_TYPE)\n",
        "\n",
        "df[\"OCCUPATION_TYPE\"] = np.where(df.OCCUPATION_TYPE.isin([\"Low-skill Laborers\", \"Cooking staff\", \"Security staff\", \"Private service staff\", \"Cleaning staff\", \"Waiters/barmen staff\"]), \"Low-skill Laborers\", df.OCCUPATION_TYPE)\n",
        "df[\"OCCUPATION_TYPE\"] = np.where(df.OCCUPATION_TYPE.isin([\"IT staff\", \"High skill tech staff\"]), \"High skill tech staff\", df.OCCUPATION_TYPE)\n",
        "\n",
        "\n",
        "rare_cols = [\"NAME_TYPE_SUITE\", \"NAME_INCOME_TYPE\"]\n",
        "\n",
        "for i in rare_cols:\n",
        "    rare_encoder(df, i, rare_perc = 0.01)\n",
        "\n",
        "    \n",
        "# Categorical features with Binary encode (0 or 1; two categories)\n",
        "for bin_feature in [\"NAME_CONTRACT_TYPE\", 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
        "    df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
        "    \n",
        "    \n",
        "# Categorical features with One-Hot encode\n",
        "df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n",
        "\n",
        "\n",
        "# NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
        "df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
        "\n",
        "# Some simple new features (percentages)\n",
        "df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
        "df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
        "\n",
        "\n",
        "# EXT SOURCE MEAN FROM OTHER ASSOCIATIONS \n",
        "df[\"NEW_EXT_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "df['NEW_APP_EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
        "\n",
        "# Satın alınacak ürünün toplam kredi tutarına oranı\n",
        "df[\"NEW_GOODS_CREDIT\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"]\n",
        "\n",
        "# Kredinin yıllık ödemesinin müşterinin toplam gelirine oranı\n",
        "df['NEW_ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "\n",
        "# Ürün ile kredi ile  arasındaki farkın toplam yıllık gelire oranı\n",
        "df[\"NEW_C_GP\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) / df[\"AMT_INCOME_TOTAL\"]\n",
        "\n",
        "\n",
        "# Başvuru sırasında müşterinin gün cinsinden yaşı eksili olarak verilmiş\n",
        "# -1 ile çarpıp 365'e böldüğümüzde kaç yaşında olduğunu buluyoruz\n",
        "\n",
        "df[\"NEW_APP_AGE\"] = round(df[\"DAYS_BIRTH\"] * -1 / 365)\n",
        "\n",
        "df['NEW_INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "df['NEW_PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
        "\n",
        "# kredinin çekildiği ürünün fiyatı / kredi miktarı\n",
        "df[\"NEW_APP_GOODS/AMT_CREDIT\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"]\n",
        "\n",
        "df['NEW_LOAN_VALUE_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
        "\n",
        "df['NEW_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "df['NEW_ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "\n",
        "df['NEW_INCOME_PER_PERSON_PERC_PAYMENT_RATE_INCOME_PER_PERSON'] = df['NEW_INCOME_PER_PERSON'] / df['NEW_PAYMENT_RATE']\n",
        "\n",
        "print(\"APPLICATION TRAIN/TEST SHAPE:\", df.shape, \"\\n\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_UPoh0dZzMA"
      },
      "source": [
        "préparation données (HomeCredit Default Risk Step by Step: 2nd Notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S6b4SiBZzlg"
      },
      "source": [
        "# df = pd.read_feather(\"../input/homecredit-default-risk-step-by-step-1st-notebook/applications_traintest_feather\")\n",
        "# pos = pd.read_feather(\"../input/homecredit-default-risk-step-by-step-1st-notebook/poscashbalance_agg_feather\")\n",
        "# bb = pd.read_feather(\"../input/homecredit-default-risk-step-by-step-1st-notebook/bureau_bureaubalance_agg_feather\")\n",
        "# cc = pd.read_feather(\"../input/homecredit-default-risk-step-by-step-1st-notebook/cc_feather\")\n",
        "# ins = pd.read_feather(\"../input/homecredit-default-risk-step-by-step-1st-notebook/installments_payments_agg_feather\")\n",
        "prev = pd.read_feather(\"../input/homecredit-default-risk-step-by-step-1st-notebook/previous_applications_agg_feather\")\n",
        "\n",
        "print(df.shape, pos.shape, bb.shape, cc.shape, ins.shape, prev.shape)\n",
        "\n",
        "for i in [pos, bb, cc, ins, prev]:\n",
        "    df = pd.merge(df, i , how = \"left\", on = \"SK_ID_CURR\")\n",
        "    \n",
        "print(df.shape)\n",
        "\n",
        "del pos, bb, ins, cc, prev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuVskX3JsJoJ"
      },
      "source": [
        "train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q7Ge9oFsLbn"
      },
      "source": [
        "# Train-Test Split\n",
        "df.columns = list(map(lambda x: str(x).replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"_/_\", \"_\").upper(), df.columns))\n",
        "import re\n",
        "df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "\n",
        "train = df[df.TARGET.isnull() == False]\n",
        "test = df[df.TARGET.isnull()]\n",
        "\n",
        "x_train = train.drop([\"TARGET\", \"SK_ID_CURR\"], axis = 1)\n",
        "x_test = test.drop([\"TARGET\", \"SK_ID_CURR\"], axis = 1)\n",
        "y_train = train.TARGET"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}